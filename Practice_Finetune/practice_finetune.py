# -*- coding: utf-8 -*-
"""Practice_Finetune.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oSWkboyXhK3g1Xk8gdNXyjwVYEEOlMci
"""

pip install transformers datasets

from datasets import load_dataset
from transformers import AutoTokenizer

model_name = "gpt2"
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token  # or '[PAD]' if you prefer a different token

# Load your plain text data
dataset = load_dataset('text', data_files='/content/data.txt')

# Tokenize function
def tokenize_function(example):
    # the labels are the same as the inputs shifted by one
    # for causal language modeling
    return tokenizer(example["text"], truncation=True, padding="max_length", max_length=128, return_tensors='pt')

tokenized_dataset = dataset.map(tokenize_function, batched=True)
tokenized_dataset = tokenized_dataset.map(lambda examples: {'labels': examples['input_ids']}, batched=True) # Setting labels to input_ids for causal LM training

from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(model_name)

from transformers import Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir="./gpt2-finetuned",
    evaluation_strategy="no",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    save_steps=500,
    save_total_limit=2,
    logging_steps=50,
    learning_rate=5e-5,
    weight_decay=0.01,
    push_to_hub=False,
    report_to="none"
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset['train']
)

from huggingface_hub import login

login("")  # paste your token inside the quotes

trainer.train()

model.save_pretrained("./gpt2-finetuned")
tokenizer.save_pretrained("./gpt2-finetuned")

from transformers import pipeline

generator = pipeline("text-generation", model="./gpt2-finetuned", tokenizer="./gpt2-finetuned")
print(generator("Neurons are", max_length=50))